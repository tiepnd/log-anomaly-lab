# Code Implementation

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
code/
â”œâ”€â”€ preprocessing/          # Log parsing vÃ  preprocessing
â”‚   â”œâ”€â”€ log_parser.py      # Main parser sá»­ dá»¥ng Drain3
â”‚   â”œâ”€â”€ test_parser.py     # Test script (sample nhá»)
â”‚   â””â”€â”€ parse_full_dataset.py  # Parse full dataset (Colab)
â”œâ”€â”€ models/                 # Model implementations
â”‚   â”œâ”€â”€ autoencoder.py
â”‚   â””â”€â”€ logbert.py
â”œâ”€â”€ training/               # Training scripts
â”‚   â”œâ”€â”€ train_autoencoder.py
â”‚   â”œâ”€â”€ train_logbert.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # File nÃ y
```

---

## ğŸš€ Setup

### 1. CÃ i Äáº·t Dependencies

```bash
# Táº¡o virtual environment
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# hoáº·c venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t packages
pip install -r requirements.txt
```

### 2. Kiá»ƒm Tra Dependencies

```bash
python3 -c "import drain3; print('drain3 OK')"
python3 -c "import torch; print('PyTorch OK')"
python3 -c "import transformers; print('Transformers OK')"
```

---

## ğŸ“ Log Parser

### Test vá»›i Sample Nhá» (Local)

```bash
cd preprocessing
python3 test_parser.py
```

**Output:**
- `hdfs_parsed_sample.json` - HDFS parsed logs (2000 dÃ²ng)
- `bgl_parsed_sample.json` - BGL parsed logs (2000 dÃ²ng)
- Statistics in console

### Parse Full Dataset (Colab)

```bash
# Upload code lÃªn Colab
# Cháº¡y:
python3 preprocessing/parse_full_dataset.py
```

**Output:**
- `preprocessing/output/hdfs_parsed_full.json`
- `preprocessing/output/bgl_parsed_full.json`
- `preprocessing/output/hdfs_parsing_stats.json`
- `preprocessing/output/bgl_parsing_stats.json`

---

## ğŸ“Š Usage

### Sá»­ dá»¥ng LogParser trong code

```python
from preprocessing.log_parser import LogParser

# Khá»Ÿi táº¡o parser
parser = LogParser()

# Parse má»™t log entry
log_line = "081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block..."
result = parser.parse_log(log_line, dataset_type="HDFS")

if result:
    print(f"Template: {result['template']}")
    print(f"Parameters: {result['parameters']}")

# Parse dataset
results = parser.parse_dataset(
    "datasets/HDFS.log",
    dataset_type="HDFS",
    max_lines=2000  # None = parse táº¥t cáº£
)

# Xem statistics
stats = parser.get_statistics()
print(f"Success rate: {stats['success_rate']:.2f}%")
```

---

## âœ… Checklist

- [x] Setup mÃ´i trÆ°á»ng
- [x] Táº£i dataset (HDFS, BGL)
- [ ] Test parser vá»›i sample nhá»
- [ ] Parse full dataset trÃªn Colab
- [ ] Implement tokenization
- [ ] Implement embedding
- [ ] Implement Autoencoder
- [ ] Implement LogBERT
- [ ] Training scripts
- [ ] Evaluation scripts

---

## ğŸ”— LiÃªn Káº¿t

- [Drain3 Documentation](https://github.com/IBM/Drain3)
- [LogHub Datasets](https://github.com/logpai/loghub)

