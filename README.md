# Code Repository - Log Anomaly Detection System

> Complete implementation of AI-based log anomaly detection system using Autoencoder and LogBERT models

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [Tá»•ng Quan](#-tá»•ng-quan)
2. [Cáº¥u TrÃºc Dá»± Ãn](#-cáº¥u-trÃºc-dá»±-Ã¡n)
3. [Quick Start](#-quick-start)
4. [Modules](#-modules)
5. [Workflow](#-workflow)
6. [Dependencies](#-dependencies)
7. [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Tá»•ng Quan

Repository nÃ y chá»©a toÃ n bá»™ code implementation cho há»‡ thá»‘ng phÃ¡t hiá»‡n báº¥t thÆ°á»ng trong log sá»­ dá»¥ng AI, bao gá»“m:

- **Preprocessing**: Log parsing, tokenization, vÃ  embedding
- **Training**: Model training, hyperparameter tuning, vÃ  threshold selection
- **Evaluation**: Metrics calculation vÃ  visualization
- **Deployment**: Realtime pipeline vá»›i Kafka, Docker, vÃ  microservices

### Models

- **Autoencoder**: Baseline model cho unsupervised anomaly detection
- **LogBERT**: Transformer-based model cho log anomaly detection

### Datasets

- **HDFS**: Hadoop Distributed File System logs (~11M entries)
- **BGL**: Blue Gene/L supercomputer logs (~4.7M entries)

---

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
code/
â”œâ”€â”€ README.md                    # File nÃ y
â”‚
â”œâ”€â”€ preprocessing/               # Module 1: Data Preprocessing
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ core/                   # Core modules (parser, tokenizer, embedder, pipeline)
â”‚   â”œâ”€â”€ scripts/                # Scripts (test, parse full dataset)
â”‚   â””â”€â”€ output/                 # Output files (parsed logs, embeddings)
â”‚
â”œâ”€â”€ training/                    # Module 2: Model Training
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ core/                   # Core modules (datasets, data_loader, training_utils, model_loader)
â”‚   â”œâ”€â”€ scripts/                # Scripts (train, test, tune, threshold, plot)
â”‚   â””â”€â”€ output/                 # Output files (checkpoints, thresholds, tuning results)
â”‚
â”œâ”€â”€ evaluation/                  # Module 3: Model Evaluation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ core/                   # Core modules (model_loader, metrics, label_loader, plotting)
â”‚   â”œâ”€â”€ scripts/                # Scripts (evaluate, plot, generate_chapter3_results)
â”‚   â””â”€â”€ output/                 # Output files (evaluation results, figures)
â”‚
â”œâ”€â”€ deployment/                  # Module 4: System Deployment
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ docker-compose.yml      # Docker Compose configuration
â”‚   â”œâ”€â”€ Dockerfile              # Docker image definition
â”‚   â”œâ”€â”€ config/                 # Configuration files
â”‚   â””â”€â”€ src/                    # Source code (services, dashboard)
â”‚
â”œâ”€â”€ models/                      # Model implementations
â”‚   â”œâ”€â”€ autoencoder.py          # Autoencoder model
â”‚   â””â”€â”€ logbert.py              # LogBERT model
â”‚
â””â”€â”€ venv/                        # Virtual environment (gitignored)
```

---

## ğŸš€ Quick Start

### BÆ°á»›c 1: Setup MÃ´i TrÆ°á»ng

```bash
# Clone repository (náº¿u chÆ°a cÃ³)
cd "master's thesis/code"

# Táº¡o virtual environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c: venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

### BÆ°á»›c 2: Preprocessing

```bash
cd preprocessing
python3 scripts/test_preprocessing.py --section all --log_file ../datasets/HDFS_2k.log
```

### BÆ°á»›c 3: Training

```bash
cd training
python3 scripts/train.py --model autoencoder --local --dataset HDFS --epochs 50
python3 scripts/train.py --model logbert --dataset HDFS --epochs 3
```

### BÆ°á»›c 4: Evaluation

```bash
cd evaluation
python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS
python3 scripts/plot.py --plot_type all --model_type both
```

---

## ğŸ“¦ Modules

### 1. Preprocessing (`preprocessing/`)

**Má»¥c Ä‘Ã­ch**: Parse raw logs, tokenize, vÃ  táº¡o embeddings

**Core Components**:
- `core/parser.py`: Log parsing vá»›i Drain3
- `core/tokenizer.py`: Word-level vÃ  BERT tokenization
- `core/embedder.py`: TF-IDF, Word2Vec, FastText, BERT embeddings
- `core/pipeline.py`: End-to-end preprocessing pipeline

**Usage**:
```bash
cd preprocessing
python3 scripts/test_preprocessing.py --section parsing --log_file ../datasets/HDFS_2k.log
```

**Output**: Parsed logs, tokenized sequences, embeddings

**Documentation**: Xem `preprocessing/README.md`

---

### 2. Training (`training/`)

**Má»¥c Ä‘Ã­ch**: Train Autoencoder vÃ  LogBERT models

**Core Components**:
- `core/datasets.py`: Dataset classes
- `core/data_loader.py`: Data loading utilities
- `core/training_utils.py`: Training loops vÃ  plotting
- `core/model_loader.py`: Model loading utilities

**Scripts**:
- `scripts/train.py`: Unified training script
- `scripts/test.py`: Model testing
- `scripts/tune.py`: Hyperparameter tuning
- `scripts/threshold.py`: Threshold selection
- `scripts/plot_tuning.py`: Visualize tuning results

**Usage**:
```bash
cd training
# Train Autoencoder
python3 scripts/train.py --model autoencoder --local --dataset HDFS --epochs 50

# Train LogBERT
python3 scripts/train.py --model logbert --dataset HDFS --epochs 3

# Tune hyperparameters
python3 scripts/tune.py --model autoencoder --dataset HDFS

# Select threshold
python3 scripts/threshold.py --model_type autoencoder --dataset HDFS
```

**Output**: Model checkpoints, training history, thresholds, tuning results

**Documentation**: Xem `training/README.md`

---

### 3. Evaluation (`evaluation/`)

**Má»¥c Ä‘Ã­ch**: Evaluate models vá»›i metrics vÃ  visualizations

**Core Components**:
- `core/model_loader.py`: Model loading
- `core/metrics.py`: Metrics calculation (Precision, Recall, F1, ROC-AUC, etc.)
- `core/label_loader.py`: Ground truth label loading vÃ  mapping
- `core/plotting.py`: Plotting utilities (ROC curves, confusion matrices, loss curves)
- `core/threshold_loader.py`: Threshold loading

**Scripts**:
- `scripts/evaluate.py`: Unified evaluation script
- `scripts/plot.py`: Unified plotting script
- `scripts/generate_chapter3_charts.py`: Generate Chapter 3 charts
- `scripts/generate_chapter3_results.py`: Generate Chapter 3 results

**Usage**:
```bash
cd evaluation
# Evaluate vá»›i ground truth labels
python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS

# Plot all results
python3 scripts/plot.py --plot_type all --model_type both --dataset both
```

**Output**: Evaluation results (JSON), ROC curves, confusion matrices, loss curves

**Documentation**: Xem `evaluation/README.md`

---

### 4. Deployment (`deployment/`)

**Má»¥c Ä‘Ã­ch**: Deploy realtime anomaly detection pipeline

**Components**:
- `docker-compose.yml`: Docker Compose configuration
- `src/log_producer.py`: Log producer service
- `src/preprocessor.py`: Log preprocessor service
- `src/model_service.py`: Model inference service
- `src/alert_service.py`: Alert service
- `src/dashboard/`: Web dashboard

**Usage**:
```bash
cd deployment
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

**Output**: Realtime anomaly detection pipeline vá»›i Kafka, services, vÃ  dashboard

**Documentation**: Xem `deployment/README.md`

---

## ğŸ”„ Workflow

### Complete Pipeline

```
1. Preprocessing
   â””â”€> Parse logs â†’ Tokenize â†’ Embed
   
2. Training
   â””â”€> Train models â†’ Tune hyperparameters â†’ Select thresholds
   
3. Evaluation
   â””â”€> Evaluate models â†’ Calculate metrics â†’ Generate visualizations
   
4. Deployment
   â””â”€> Deploy services â†’ Run realtime pipeline â†’ Monitor dashboard
```

### Recommended Workflow

1. **Local Testing** (Small datasets: `HDFS_2k.log`, `BGL_2k.log`)
   ```bash
   # Preprocessing
   cd preprocessing && python3 scripts/test_preprocessing.py --section all
   
   # Training
   cd ../training && python3 scripts/train.py --model autoencoder --local --dataset HDFS
   
   # Evaluation
   cd ../evaluation && python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS
   ```

2. **Full Dataset** (Colab hoáº·c server vá»›i GPU)
   ```bash
   # Preprocessing (full dataset)
   python3 scripts/test_preprocessing.py --section all --parse_full
   
   # Training (full dataset)
   python3 scripts/train.py --model autoencoder --input_file ../preprocessing/output/processed/hdfs_processed.json
   
   # Evaluation (full dataset)
   python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS
   ```

3. **Production Deployment**
   ```bash
   cd deployment
   docker-compose up -d
   ```

---

## ğŸ“š Dependencies

### Core Dependencies

```txt
torch>=1.12.0
transformers>=4.20.0
scikit-learn>=1.0.0
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.5.0
seaborn>=0.11.0
tqdm>=4.64.0
```

### Preprocessing Dependencies

```txt
drain3>=0.9.2
gensim>=4.0.0
```

### Deployment Dependencies

```txt
kafka-python>=2.0.2
flask>=2.0.0
fastapi>=0.75.0
docker>=5.0.0
docker-compose>=1.29.0
```

### Installation

```bash
# Install all dependencies
pip install -r requirements.txt

# Or install by module
pip install -r preprocessing/requirements.txt
pip install -r training/requirements.txt
pip install -r evaluation/requirements.txt
pip install -r deployment/requirements.txt
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. ModuleNotFoundError

**Error**: `ModuleNotFoundError: No module named 'preprocessing'`

**Solution**:
```bash
# Äáº£m báº£o Ä‘ang á»Ÿ Ä‘Ãºng thÆ° má»¥c
cd code/preprocessing
source ../venv/bin/activate
python3 scripts/test_preprocessing.py
```

#### 2. CUDA Out of Memory

**Error**: `RuntimeError: CUDA out of memory`

**Solution**:
- Giáº£m batch size: `--batch_size 8` hoáº·c `--batch_size 4`
- DÃ¹ng CPU: `--device cpu`
- DÃ¹ng DistilBERT thay vÃ¬ BERT: `--bert_model distilbert-base-uncased`

#### 3. File Not Found

**Error**: `FileNotFoundError: Checkpoint not found`

**Solution**:
- Kiá»ƒm tra paths: `../training/output/checkpoints/autoencoder_hdfs_local/best_model.pt`
- Äáº£m báº£o Ä‘Ã£ train model trÆ°á»›c khi evaluate

#### 4. Import Errors

**Error**: `ImportError: cannot import name 'X' from 'Y'`

**Solution**:
```bash
# Reinstall dependencies
pip install --upgrade -r requirements.txt

# Check Python version (requires Python 3.8+)
python3 --version
```

#### 5. Docker Issues

**Error**: `docker-compose: command not found`

**Solution**:
```bash
# Install Docker Compose
pip install docker-compose

# Or use docker compose (newer versions)
docker compose up -d
```

---

## ğŸ“Š Datasets

### HDFS Dataset

- **File**: `datasets/HDFS.log` (full) hoáº·c `datasets/HDFS_2k.log` (local)
- **Size**: ~11M entries (full) hoáº·c ~2000 entries (local)
- **Format**: `081109 203518 143 INFO dfs.DataNode: ...`
- **Labels**: `datasets/HDFS_v1/preprocessed/anomaly_label.csv`

### BGL Dataset

- **File**: `datasets/BGL.log` (full) hoáº·c `datasets/BGL_2k.log` (local)
- **Size**: ~4.7M entries (full) hoáº·c ~2000 entries (local)
- **Format**: `- 1117838570 2005.06.03 R02-M1-N0-C:J12-U11 ... RAS KERNEL INFO ...`

---

## ğŸ¯ Next Steps

1. âœ… **Preprocessing** - Parse vÃ  embed logs
2. âœ… **Training** - Train Autoencoder vÃ  LogBERT
3. âœ… **Evaluation** - Evaluate vá»›i metrics vÃ  visualizations
4. â­ï¸ **Deployment** - Deploy realtime pipeline
5. â­ï¸ **Monitoring** - Monitor system performance

---

## ğŸ“– Documentation

- **Preprocessing**: `preprocessing/README.md`
- **Training**: `training/README.md`
- **Evaluation**: `evaluation/README.md`
- **Deployment**: `deployment/README.md`

---

## ğŸ’¡ Tips

1. **Local Testing**: LuÃ´n test vá»›i small datasets (`*_2k.log`) trÆ°á»›c khi cháº¡y full dataset
2. **GPU Usage**: Sá»­ dá»¥ng GPU cho training vÃ  evaluation náº¿u cÃ³ (cáº§n ~8GB VRAM cho BERT)
3. **Memory Management**: Giáº£m batch size náº¿u gáº·p out-of-memory errors
4. **Checkpoints**: Models Ä‘Æ°á»£c tá»± Ä‘á»™ng save, cÃ³ thá»ƒ resume training tá»« checkpoint
5. **Logging**: Táº¥t cáº£ modules Ä‘á»u cÃ³ logging, check logs Ä‘á»ƒ debug

---

## ğŸ”— Related Files

- **Thesis Chapters**: `chapters/chapter_02/`, `chapters/chapter_03/`
- **Figures**: `figures/chapter_02/`, `figures/chapter_03/`
- **Tables**: `tables/chapter_02/`, `tables/chapter_03/`
- **References**: `references/references.bib`

---

## ğŸ“ License

This code is part of a Master's thesis project. All rights reserved.

---

## ğŸ‘¤ Author

Master's Thesis - Nguyen Duc Tiep

---

**Happy Coding! ğŸš€**
