# Evaluation Module - HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

> Evaluation, metrics calculation, vÃ  visualization cho Autoencoder vÃ  LogBERT models

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [Quick Start](#-quick-start)
2. [Cáº¥u TrÃºc Module](#-cáº¥u-trÃºc-module)
3. [Core Modules](#-core-modules)
4. [Evaluation](#-evaluation)
5. [Plotting](#-plotting)
6. [Evaluation vá»›i Ground Truth Labels](#-evaluation-vá»›i-ground-truth-labels)
7. [Chapter 3 Results](#-chapter-3-results)
8. [Troubleshooting](#-troubleshooting)

---

## ğŸš€ Quick Start

### BÆ°á»›c 1: Evaluate Autoencoder

```bash
cd code/evaluation
source ../venv/bin/activate
python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS
```

**Output:** `output/evaluation/autoencoder_evaluation.json`

### BÆ°á»›c 2: Evaluate LogBERT

```bash
python3 scripts/evaluate.py --model_type logbert --dataset HDFS
```

**Output:** `output/evaluation/logbert_evaluation.json`

### BÆ°á»›c 3: Plot Results

```bash
python3 scripts/plot.py --plot_type all --model_type both --dataset both
```

**Output:** `figures/chapter_02/roc_curve_*.png`, `confusion_matrix_*.png`, `*_loss_curve_*.png`

---

## ğŸ“ Cáº¥u TrÃºc Module

```
evaluation/
â”œâ”€â”€ README.md                    # File nÃ y
â”‚
â”œâ”€â”€ core/                        # âœ… Core utilities (shared code)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_loader.py         # Model loading functions
â”‚   â”œâ”€â”€ label_loader.py         # Label loading and mapping
â”‚   â”œâ”€â”€ metrics.py              # Metrics calculation
â”‚   â”œâ”€â”€ plotting.py             # Plotting functions
â”‚   â””â”€â”€ threshold_loader.py     # Threshold loading
â”‚
â”œâ”€â”€ scripts/                     # âœ… Táº¥t cáº£ scripts Ä‘á»ƒ cháº¡y
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate.py             # âœ… Unified evaluation script
â”‚   â”œâ”€â”€ plot.py                 # âœ… Unified plotting script
â”‚   â”œâ”€â”€ generate_chapter3_charts.py    # Generate Chapter 3 charts
â”‚   â””â”€â”€ generate_chapter3_results.py   # Generate Chapter 3 results
â”‚
â””â”€â”€ output/                      # Táº¥t cáº£ output files
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ evaluation/              # Evaluation results JSON
    â””â”€â”€ drain3_state/            # Drain3 state files
```

### âœ¨ Äiá»ƒm Ná»•i Báº­t

- **Unified Scripts**: Má»™t script cho nhiá»u use cases (`evaluate.py`, `plot.py`)
- **Core Modules**: Shared code Ä‘Æ°á»£c extract vÃ o `core/` Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p
- **Organized Output**: Táº¥t cáº£ outputs Ä‘Æ°á»£c tá»• chá»©c trong `output/`

---

## ğŸ”§ Core Modules

### `core/model_loader.py`

Model loading utilities:

- **`load_autoencoder_model()`**: Load Autoencoder tá»« checkpoint
- **`load_logbert_model()`**: Load LogBERT tá»« checkpoint

**Usage:**
```python
from evaluation.core import load_autoencoder_model, load_logbert_model

# Load Autoencoder
model, config = load_autoencoder_model("path/to/checkpoint.pt", device="cpu")

# Load LogBERT
model, config = load_logbert_model("path/to/checkpoint.pt", device="cpu")
```

### `core/label_loader.py`

Label loading and mapping utilities:

- **`load_ground_truth_labels()`**: Load labels tá»« CSV file
- **`extract_block_id_from_log()`**: Extract block ID tá»« log entry
- **`map_logs_to_labels()`**: Map log entries vá»›i labels

**Usage:**
```python
from evaluation.core import (
    load_ground_truth_labels,
    map_logs_to_labels
)

# Load labels
labels_dict = load_ground_truth_labels("path/to/labels.csv")

# Map logs to labels
indices, labels = map_logs_to_labels(parsed_logs, labels_dict)
```

### `core/metrics.py`

Metrics calculation utilities:

- **`calculate_metrics()`**: Calculate all metrics (Precision, Recall, F1, ROC-AUC, etc.)
- **`get_roc_curve()`**: Calculate ROC curve
- **`get_pr_curve()`**: Calculate Precision-Recall curve

**Usage:**
```python
from evaluation.core import calculate_metrics, get_roc_curve

# Calculate metrics
metrics = calculate_metrics(y_true, y_pred, y_scores)

# Get ROC curve
fpr, tpr, roc_auc = get_roc_curve(y_true, y_scores)
```

### `core/plotting.py`

Plotting utilities:

- **`plot_roc_curve()`**: Plot ROC curve
- **`plot_confusion_matrix()`**: Plot confusion matrix
- **`plot_loss_curve()`**: Plot loss curves
- **`plot_score_distribution()`**: Plot score/error distribution

**Usage:**
```python
from evaluation.core import (
    plot_roc_curve,
    plot_confusion_matrix,
    plot_loss_curve
)

# Plot ROC curve
plot_roc_curve(fpr, tpr, roc_auc, "Model Name", save_path)

# Plot confusion matrix
plot_confusion_matrix(y_true, y_pred, "Model Name", save_path, metrics)
```

### `core/threshold_loader.py`

Threshold loading utilities:

- **`load_threshold()`**: Load threshold tá»« JSON file

**Usage:**
```python
from evaluation.core import load_threshold

threshold = load_threshold("autoencoder", "HDFS", threshold_dir="path/to/thresholds")
```

---

## ğŸ¯ Evaluation

### Evaluate Autoencoder

```bash
python3 scripts/evaluate.py --model_type autoencoder \
    --dataset HDFS \
    --checkpoint_dir ../training/output/checkpoints \
    --threshold_dir ../training/output/thresholds \
    --device cpu \
    --save_dir output/evaluation
```

**Arguments:**
- `--model_type`: `autoencoder` hoáº·c `logbert` (required)
- `--dataset`: `HDFS` hoáº·c `BGL` (default: HDFS)
- `--checkpoint_dir`: Directory chá»©a checkpoints (default: `../training/output/checkpoints`)
- `--threshold_dir`: Directory chá»©a thresholds (default: `../training/output/thresholds`)
- `--log_file`: Path to log file (auto-detect náº¿u None)
- `--label_file`: Path to ground truth labels CSV (auto-detect cho HDFS náº¿u None)
- `--device`: `cpu` hoáº·c `cuda` (default: cpu)
- `--save_dir`: Directory Ä‘á»ƒ lÆ°u results (default: `output/evaluation`)

**Output Files:**
- `output/evaluation/autoencoder_evaluation.json` - Evaluation results
- `output/evaluation/roc_curve_autoencoder.png` - ROC curve (náº¿u cÃ³ labels)
- `output/evaluation/confusion_matrix_autoencoder.png` - Confusion matrix (náº¿u cÃ³ labels)

### Evaluate LogBERT

```bash
python3 scripts/evaluate.py --model_type logbert \
    --dataset HDFS \
    --bert_model distilbert-base-uncased \
    --device cpu
```

**Arguments:**
- `--bert_model`: BERT model name (default: `distilbert-base-uncased`)

**Output Files:**
- `output/evaluation/logbert_evaluation.json` - Evaluation results
- `output/evaluation/roc_curve_logbert.png` - ROC curve (náº¿u cÃ³ labels)
- `output/evaluation/confusion_matrix_logbert.png` - Confusion matrix (náº¿u cÃ³ labels)

---

## ğŸ“Š Plotting

### Plot All Results

```bash
python3 scripts/plot.py --plot_type all \
    --model_type both \
    --dataset both \
    --evaluation_dir output/evaluation \
    --figures_dir ../../figures/chapter_02
```

**Arguments:**
- `--plot_type`: `roc`, `cm`, `loss`, hoáº·c `all` (default: all)
- `--model_type`: `autoencoder`, `logbert`, hoáº·c `both` (default: both)
- `--dataset`: `HDFS`, `BGL`, hoáº·c `both` (default: both)
- `--evaluation_dir`: Directory chá»©a evaluation results (default: `output/evaluation`)
- `--checkpoint_dir`: Directory chá»©a checkpoints (default: `../training/output/checkpoints`)
- `--figures_dir`: Directory Ä‘á»ƒ lÆ°u figures (default: `../../figures/chapter_02`)

**Output Files:**
- `figures/chapter_02/roc_curve_autoencoder_hdfs.png`
- `figures/chapter_02/confusion_matrix_autoencoder_hdfs.png`
- `figures/chapter_02/autoencoder_loss_curve_hdfs.png`
- `figures/chapter_02/roc_curve_logbert_hdfs.png`
- `figures/chapter_02/confusion_matrix_logbert_hdfs.png`
- `figures/chapter_02/logbert_loss_curve_hdfs.png`

---

## ğŸ“ Evaluation vá»›i Ground Truth Labels

### HDFS Dataset Labels

- **File**: `datasets/HDFS_v1/preprocessed/anomaly_label.csv`
- **Format**: CSV vá»›i columns `BlockId,Label`
- **Labels**: "Normal" hoáº·c "Anomaly"
- **Total**: ~575,061 labels (558,223 Normal, 16,838 Anomaly)

### Auto-detect Labels (HDFS)

```bash
# Auto-detect labels cho HDFS
python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS
```

### Manual Label File

```bash
# Specify label file manually
python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS \
    --label_file /path/to/labels.csv
```

### Label Mapping Process

1. **Load Labels**: Load labels tá»« CSV file
2. **Extract Block IDs**: Extract block IDs tá»« log entries (pattern: `blk_<number>`)
3. **Map Logs**: Map log entries vá»›i labels dá»±a trÃªn block IDs
4. **Filter**: Filter embeddings/templates cÃ³ labels
5. **Evaluate**: Evaluate vá»›i ground truth labels

### Metrics vá»›i Labels

Khi cÃ³ ground truth labels, evaluation sáº½ tÃ­nh:

- **Precision**: TP / (TP + FP)
- **Recall**: TP / (TP + FN)
- **F1-Score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
- **Accuracy**: (TP + TN) / (TP + TN + FP + FN)
- **ROC-AUC**: Area Under ROC Curve
- **PR-AUC**: Area Under Precision-Recall Curve
- **Confusion Matrix**: TP, FP, TN, FN

---

## ğŸ“ˆ Chapter 3 Results

### Generate Chapter 3 Charts

```bash
python3 scripts/generate_chapter3_charts.py
```

**Output:**
- `figures/chapter_03/throughput_vs_load.png` - Throughput vs Load chart
- `figures/chapter_03/latency_distribution.png` - Latency Distribution chart

### Generate Chapter 3 Results (Full)

```bash
python3 scripts/generate_chapter3_results.py
```

**Output:**
- `tables/chapter_03/performance_metrics.md` - Performance metrics table
- `tables/chapter_03/model_comparison.md` - Model comparison tables
- Charts (same as above)

---

## ğŸ“Š Evaluation Metrics

### Metrics TÃ­nh ToÃ¡n

1. **Precision**: TP / (TP + FP)
   - Tá»· lá»‡ log Ä‘Æ°á»£c dá»± Ä‘oÃ¡n lÃ  anomaly thá»±c sá»± lÃ  anomaly

2. **Recall**: TP / (TP + FN)
   - Tá»· lá»‡ anomaly Ä‘Æ°á»£c phÃ¡t hiá»‡n trong tá»•ng sá»‘ anomaly thá»±c táº¿

3. **F1-Score**: 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
   - Harmonic mean cá»§a Precision vÃ  Recall

4. **Accuracy**: (TP + TN) / (TP + TN + FP + FN)
   - Tá»· lá»‡ predictions Ä‘Ãºng

5. **ROC-AUC**: Area Under ROC Curve
   - ÄÃ¡nh giÃ¡ kháº£ nÄƒng phÃ¢n loáº¡i á»Ÿ cÃ¡c threshold khÃ¡c nhau

6. **PR-AUC**: Area Under Precision-Recall Curve
   - ÄÃ¡nh giÃ¡ performance khi cÃ³ class imbalance

### Confusion Matrix

- **TP (True Positives)**: Anomaly Ä‘Æ°á»£c phÃ¡t hiá»‡n Ä‘Ãºng
- **FP (False Positives)**: Normal bá»‹ phÃ¢n loáº¡i nháº§m lÃ  anomaly
- **TN (True Negatives)**: Normal Ä‘Æ°á»£c phÃ¢n loáº¡i Ä‘Ãºng
- **FN (False Negatives)**: Anomaly bá»‹ bá» sÃ³t

---

## ğŸ” Unsupervised Evaluation

Khi khÃ´ng cÃ³ ground truth labels (unsupervised learning), evaluation chá»‰ cÃ³ thá»ƒ:

- TÃ­nh reconstruction errors / anomaly scores
- TÃ­nh prediction statistics (anomaly_count, anomaly_rate)
- Plot score/error distributions

**Äá»ƒ cÃ³ full metrics, cáº§n:**
- Test set cÃ³ labels (ground truth)
- Hoáº·c manual labeling má»™t subset

---

## ğŸ”§ Troubleshooting

### Lá»—i: "ModuleNotFoundError: No module named 'evaluation'"

**Giáº£i phÃ¡p:**
```bash
# Äáº£m báº£o Ä‘ang á»Ÿ Ä‘Ãºng thÆ° má»¥c
cd code/evaluation
source ../venv/bin/activate
python3 scripts/evaluate.py --model_type autoencoder --dataset HDFS
```

### Lá»—i: "FileNotFoundError: Checkpoint not found"

**Giáº£i phÃ¡p:**
- Kiá»ƒm tra checkpoint path: `../training/output/checkpoints/autoencoder_hdfs_local/best_model.pt`
- Äáº£m báº£o Ä‘Ã£ train model trÆ°á»›c khi evaluate

### Lá»—i: "Threshold file not found"

**Giáº£i phÃ¡p:**
- Kiá»ƒm tra threshold path: `../training/output/thresholds/autoencoder_threshold.json`
- Äáº£m báº£o Ä‘Ã£ select threshold trÆ°á»›c khi evaluate

### Lá»—i: "Label file not found"

**Giáº£i phÃ¡p:**
- Kiá»ƒm tra label file path: `datasets/HDFS_v1/preprocessed/anomaly_label.csv`
- Hoáº·c specify label file manually: `--label_file /path/to/labels.csv`

### Lá»—i: "CUDA out of memory"

**Giáº£i phÃ¡p:**
- DÃ¹ng CPU: `--device cpu`
- Giáº£m batch size trong code (náº¿u cÃ³ thá»ƒ)

---

## ğŸ’¡ Tips

1. **Local Testing:** LuÃ´n test vá»›i `HDFS_2k.log` trÆ°á»›c khi evaluate full dataset
2. **Save Results:** Evaluation results Ä‘Æ°á»£c tá»± Ä‘á»™ng save vÃ o `output/evaluation/`
3. **Labels:** Evaluation vá»›i labels cho káº¿t quáº£ chÃ­nh xÃ¡c hÆ¡n
4. **Plotting:** DÃ¹ng `scripts/plot.py` Ä‘á»ƒ plot táº¥t cáº£ results cÃ¹ng lÃºc
5. **Core Modules:** Sá»­ dá»¥ng core modules Ä‘á»ƒ trÃ¡nh code trÃ¹ng láº·p

---

## ğŸ¯ Next Steps

Sau khi evaluation:
1. âœ… **Evaluation metrics** - HoÃ n thÃ nh
2. âœ… **ROC curves** - HoÃ n thÃ nh
3. âœ… **Confusion matrices** - HoÃ n thÃ nh
4. âœ… **Loss curves** - HoÃ n thÃ nh
5. â­ï¸ **Compare results** - So sÃ¡nh Autoencoder vs LogBERT
6. â­ï¸ **Analyze errors** - PhÃ¢n tÃ­ch false positives/negatives

---

## ğŸ“š References

- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [ROC Curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)
- [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)

---

**Happy Evaluating! ğŸš€**
